{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next:\n",
    "\n",
    "Linear Regression with Linear Algebra\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "car_df = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Auto.csv',na_values='?').dropna()\n",
    "car_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = car_df[['cylinders','displacement','horsepower','weight','acceleration','year']]\n",
    "y= car_df['mpg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_array = np.ones(len(X_df.cylinders))\n",
    "X_df['constant'] = one_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our linear regression equation can be represented as:\n",
    "\n",
    "$ y = Xb + error $\n",
    "\n",
    "to begin with, we are going to assume that on average, the error is equal to 0. (We can assume this due our assumptions for linear algebra)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y = Xb  + 0 $   --> $ y = Xb $\n",
    "\n",
    "We want to solve for b. That means we need to isolate it on one side of the equation therefore we can take the inverse of this function. Let's multily by the inverse. Let's calculate it:\n",
    "\n",
    "$ X^{-1} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can only calculate an inverse of a **square** matrix. We can make accomplish this be multiplying the matrix by its transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X^{T} y = X^{T} X b $\n",
    "\n",
    "Now we can take the inverse of the new square matrix $X^{T}X$ and multiply it to both sides to isolate with b vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ (X^{T}X)^{-1} X^{T}y = (X^{T}X)^{-1} (X^{T}X)b$ \n",
    "\n",
    "$(X^{T}X)^{-1}X^{T}y = I b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with the identity matrix multiplied by b, which is simply equal to b\n",
    "\n",
    "$b = (X^{T}X)^{-1}X^{T}y  $\n",
    "\n",
    "Now, solve for b!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## this makes the matrix\n",
    "x_transposed = x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_X = x_transposed.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XtX_inv = np.linalg.inv(Xt_X)\n",
    "Xty = x_transposed.dot(y)\n",
    "x_hat = XtX_inv.dot(Xty) \n",
    "x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "skl_X = X_df.drop(columns = 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(skl_X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The linear regression is computed as $(X^{T}X)^{-1} X^{T}Y $\n",
    "\n",
    "If X is an (n x k) matrix:\n",
    "\n",
    "$ (X^{T} X) $ takes $ O(n*k{^2})$ time and produces a (k x k) matrix\n",
    "\n",
    "The matrix inversion of a (k x k) matrix takes $ O(k^{3}) $ time\n",
    "\n",
    "$(X^{T}Y)$ takes $O(n*k^{2})$ time and produces a (k x k) matrix\n",
    "\n",
    "The final matrix multiplication of two (k x k) matrices takes $ O(k^{3}) $ time\n",
    "\n",
    "\n",
    "Long story short, it can take a looooong time!\n",
    "\n",
    "More on O(n) notation here: https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How do we deal with such computational cost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The solution\n",
    "\n",
    "Gradient Descent! (You'll learn about this tomorrow)\n",
    "\n",
    "<img src = \"./resources/gradient_descen6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closed Form v. Gradient Descent with regression\n",
    "https://stats.stackexchange.com/questions/23128/solving-for-regression-parameters-in-closed-form-vs-gradient-descent\n",
    "\n",
    "Deriving the formula for least square estimation\n",
    "https://web.williams.edu/Mathematics/sjmiller/public_html/BrownClasses/54/handouts/MethodLeastSquares.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
